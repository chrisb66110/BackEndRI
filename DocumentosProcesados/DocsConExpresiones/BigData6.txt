 qu herramientas necesitas para iniciarte en big data inlab fib press x to close pasar al contenido principal contacto catal espaol english soluciones empresas soluciones para empresas reas especializacin colaboradores formacin proyectos herramientas citscale programa talent laboratorio docente blog inlab fib conoce inlab fib equipo formulario de bsqueda search this site qu herramientas necesitas para iniciarte en big data enviado por juan salmeron en vie 15 01 2016 13 54 introduccin desde la creacin de internet y cada vez ms se ha contado con datos generados por infinidad de aplicaciones y que tenemos a nuestro alcance esperando a ser usados antao tener y consumir todo este ingente material se poda volver una tarea prcticamente imposible o inviable tanto econmica como tecnolgicamente esto limitaba el uso de la informacin a aquella que era ms relevante y punto vamos a ver en que herramientas podemos iniciarnos para solventar este handicap y sacar el mximo valor a nuestros datos herramientas open source y que cualquiera tiene a su disposicin el ecosistema hadoop hardware hoy en da tenemos a nuestro alcance el poder consumir esta informacin sin necesidad de contar con un carsimo supercomputador en nuestro trastero podemos llegar a hacer pruebas con sistemas de 8 de ram y unos cuantos gb de disco duro en un entorno de produccin se usa comodity hardware es decir mquinas que no necesitan tener un alto grado de fiabilidad y sofisticacin raid discos duros enterprise componentes redundantes mquinas de este tipo son mucho ms baratas y si se rompen ponemos otras y ya est el peso de la fiabilidad recae sobre el software tendremos esto s que dimensionar las caractersticas segn los requerimientos de nuestro escenario una configuracin tpica de un nodo de un entorno productivo podra ser una mquina con 8 24 cores 32 de ram y unos 8 12 discos hdd para introducirnos en el uso de este software no es necesario tener un cluster de mquinas potentes podremos ponernos a trastear con un cubieboard cluster unas simples vm s en nuestro pc o con alguna vm como la cloudera quickstart vm con todo ya montado que sirve justamente para aprender motivacin antes de nada dejemos clara una cosa y no nos engaemos big data es para tratar con grandes volmenes de datos big data aparece cuando decidimos dejar de quedarnos con lo que era ms relevante y pasar a quedarnos con todo toda informacin sirve en algn momento y nos puede permitir ver cosas que con solamente no podemos llegar a ver y que en muchas ocasiones nos puede aportar mucho valor como decamos hoy en da se ha vuelto factible tener mucha informacin y ser capaz de consumirla pero tambin hay que entender cuando tiene sentido usar herramientas big data y cuando no si tenemos un volumen alto de datos y tenemos que lidiar con gigas y gigas de datos alguna magnitud estar bien usar estas herramientas de no ser as tambin podemos hacer uso de todo esto pero matar moscas a caonazos quizs no es tan adecuado aunque sea la moda la base hablar de big data es hablar de hadoop y todo lo que lo rodea vamos a ver qu es hadoop hadoop se sustenta en la forma en la que almacena y accede a los datos hadoop est formado por hdfs y mapreduce la combinacin de estos dos permite que los datos estn replicados y distribuidos por n nodos beneficiando la capacidad de acceso a grandes volmenes cuando queremos ejecutar alguna operacin sobre estos datos distribuidos hadoop se encarga de procesar cada porcin de los datos en el nodo que los contiene de esta forma se aprovecha la localidad de tener los datos cerca de donde se van a procesar y permite escalar de forma casi lineal si queremos crecer en capacidad aadimos ms nodos y listo del almacenamiento se encarga hdfs y del procesamiento mapreduce complementos bsicos con hdfs y mapreduce tenemos la capacidad bsica de almacenar datos en crudo y realizar procesos en paralelo abstrayndonos de la complejidad de este tipo de computacin ahora bien podemos utilizar otras herramientas encima de hadoop que nos potencian estas capacidades todo depender de nuestras necesidades estos son los dos ms tpicos hdfs es el filesystem en el que se basa hadoop este sistema de ficheros se basa en una arquitectura master slave donde los masternodes coordinan a los datanodes que son aquellos donde se guarda la informacin los datos en hdfs se distribuyen por los diferentes datanodes en particiones del fichero original asegurando que cada una de estas particiones est replicada en un nivel de replicacin definido por tanto no necesitamos disponer de sistemas con raid si un disco se estropea hdfs automticamente replica muy rpidamente todas las particiones que estaban en este disco en todo el resto de datanodes yarn mapreduce yarn es la evolucin de mapreduce para hadoop 2 0 la funcin de yarn en hadoop es la de proporcionar un entorno que gestione los recursos para realizar trabajos de computacin yarn se ocupa de distribuir el trabajo a hacer teniendo en cuenta donde estn los datos a procesar adems de gestionar las propias ejecuciones de los programas con yarn podemos separar el sistema de ficheros del sistema de ejecucin por tanto podemos usar hdfs sin pasar por yarn o bien usar este gestor de recursos para correr aplicaciones es el caso de la mayora de herramientas que trabajan en hbase se puede considerar base de de hadoop basado en bigtable de google proporciona la capacidad de crear tablas con millones de entradas y permite hacer accesos de lectura escritura rpida y consistentemente tambin es versionada y no relacional por lo que ofrece flexibilidad adems es fcilmente conectable ya sea a travs de su api java o mediante web services podemos usar hbase para dar forma a nuestros datos hive es una mezcla entre mapreduce y hbase permite estructurar los datos en tablas y vistas y nos permite realizar todo tipo de consultas usando su lenguaje de querying tipo sql el hiveql para aquellos casos en los que no podamos obtener lo que queremos con hiveql podemos conectar hive con nuestros propios mappers y reducers ya que hive es muy flexible en cuanto a conectividad y se le puede conectar cualquier cosa spark es el rey que ha venido a conquistarlos a todos mientras que mapreduce realiza sus procesos sobre disco spark carga en memoria los datos y realiza operaciones entre datasets intermedios llamados rdds esto hace que su rendimiento sea brutal adems es bastante sencillo de utilizar y tiene soporte tanto para scala como para java y python contando con libreras para realizar procesamiento en micro batch machine learning grafos y sql otros complementos adems de la suite tpica que hemos visto hay muchos otros productos que funcionan junto con hadoop y permiten nuevas funcionalidades segn el tipo de aplicacin usaremos unas u otras tambin existen variantes de estas herramientas adaptadas a usos especficos como sparkonhbase spork rhadoop y otros muchos ms aqu tenis una minscula clasificacin data engineering spark hive pig data discovery analytics spark impala solr data integration storage hbase kudu hdfs unified data services yarn sentry hue oozie data ingestion sqoop flume kafka hands o n hemos visto solo una pincelada de las herramientas que se usan para bigdata dominarlas es cuestin de horas y de prctica para practicar podis hacer uso de la vm que anteriormente os comentaba para poner en practica alguno de los muchos cursos y tutoriales que se pueden encontrar on line como este curso gratuito que te ensearn desde hacer el clsico wordcount hasta realizar tareas de analytics ms complejas tambin recomiendo encarecidamente echar mano de los clsicos libros o reilly especficos de cada tecnologa proyectos relacionados ra board de recursos para el barcelona virtual mobility lab a la barcelona smart shuttle pilot un termmetro emocional para la docencia uoc ndex usando learning analytics para medir el elearning big data analytics lab learning analytics learning analytics pilares artculos relacionados qu es la visualizacin de datos dataviz big data aplicado al deporte qu es un data scientist sguenos en inlab fib incorpora escert inlab es miembro de este web usa cookies propias para ofrecer una mejor experiencia y servicio al continuar con la navegacin entendemos que aceptas nuestra poltica de cookies inlab fib 93 401 69 41 c jordi girona 1 3 edificio 08034 barcelona inlab fib upc edu intranet sobre esta web 