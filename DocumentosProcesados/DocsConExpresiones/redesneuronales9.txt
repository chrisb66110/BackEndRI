 breve historia de las redes neuronales artificiales aprende machine learning ir al contenido aprende machine learning crea tu propia inteligencia artificial antes que sea demasiado tarde men inicio contacto sobre mi gu de aprendizaje publicado en 12 09 2018 26 02 2019 por breve historia de las redes neuronales artificiales arquitecturas y aplicaciones de las redes neuronales m s usadas vamos a hacer un repaso por las diversas estructuras inventadas mejoradas y utilizadas a lo largo de la historia para crear redes neuronales y sacar el mayor potencial al deep learning para resolver toda clase de problemas de regresi y clasificaci evoluci de las redes neuronales en ciencias de la computaci vamos a revisar las siguientes redes arquitecturas 1958 perceptron 1965 multilayer perceptron s neuronas sigmoidales redes feedforward backpropagation 1989 convolutional neural networks neural networks 1997 long short term memory 2006 deep belief networks nace deep learning restricted boltzmann machine encoder decoder auto encoder 2014 generative adversarial networks si bien esta lista no es exhaustiva y no se abarcan todos los modelos creados desde los a 50 he recopilado las que fueron a mi parecer las redes y tecnolog m s importantes desarrolladas para llegar al punto en que estamos hoy el aprendizaje profundo el inicio de todo la neurona artificial 1958 perceptron entre las d de 1950 y 1960 el cient frank rosenblatt inspirado en el trabajo de warren mcculloch y walter pitts cre el perceptron la unidad desde donde nacer y se potenciar las redes neuronales artificiales un perceptron toma varias entradas binarias etc y produce una s salida binaria para calcular la salida rosenblatt introduce el concepto de etc un n real que expresa la importancia de la respectiva entrada con la salida la salida de la neurona ser 1 o 0 si la suma de la multiplicaci de pesos por entradas es mayor o menor a un determinado umbral sus principales usos son decisiones binarias sencillas o para crear funciones l como or and 1965 multilayer perceptron como se imaginar n el multilayer perceptron es una amplicaci del percepci de una neurona a m s de una adem s aparece el concepto de capas de entrada oculta y salida pero con valores de entrada y salida binarios no olvidemos que tanto el valor de los pesos como el de umbral de cada neurona lo asignaba manualmente el cient cuantos m s perceptrones en las capas mucho m s dif conseguir los pesos para obtener salidas deseadas los aprendizaje autom tico neuronas sigmoides para poder lograr que las redes de neuronas aprendieran solas fue necesario introducir un nuevo tipo de neuronas las llamadas neuronas sigmoides son similares al perceptron pero permiten que las entradas en vez de ser ceros o unos puedan tener valores reales como 0 5 0 377 lo que sea tambi aparecen las neuronas que siempre suman 1 en las diversas capas para resolver ciertas situaciones ahora las salidas en vez de ser 0 1 ser x donde d ser la funci sigmoide definida como 1 1 z esta es la primer funci de activaci imagen de la curva log normalizada de wikipedia con esta nueva f se puede lograr que peque alteraciones en valores de los pesos produzcan peque alteraciones en la salida por lo tanto podemos ir ajustando muy de a poco los pesos de las conexiones e ir obteniendo las salidas deseadas redes feedforward se les llama as a las redes en que las salidas de una capa son utilizadas como entradas en la pr capa esto quiere decir que no hay loops hacia atr siempre se de valores hacia adelante hay redes que veremos m s adelante en las que s que existen esos loops neural adem s existe el concepto de fully connected feedforward y se refiere a que todas las neuronas de entrada est n conectadas con todas las neuronas de la siguiente capa 1986 backpropagation gracias al algoritmo de backpropagation se hizo posible entrenar redes neuronales de multiples capas de manera supervisada al calcular el error obtenido en la salida e ir propagando hacia las capas anteriores se van haciendo ajustes peque cada iteraci para lograr que la red aprenda consiguiendo que la red pueda por ejemplo clasificar las entradas correctamente nuevo art pron de series temporales con redes neuronales 1989 convolutional neural network las convolutional neural networks son redes multilayered que toman su inspiraci del cortex visual de los animales esta arquitectura es en varias aplicaciones principalmente procesamiento de im genes la primera cnn fue creada por yann lecun y estaba enfocada en el reconocimiento de letras manuscritas la arquitectura constaba de varias capas que implementaban la extracci de caracter y luego clasificar la imagen se divide en campos receptivos que alimentan una capa convolutional que extrae features de la imagen de entrada ejemplo detectar lineas verticales v el siguiente paso es pooling que reduce la dimensionalidad de las features extra manteniendo la informaci m s importante luego se hace una nueva convoluci y otro pooling que alimenta una red feedforward multicapa la salida final de la red es un grupo de nodos que clasifican el resultado por ejemplo un nodo para cada n del 0 al 9 decir 10 nodos se de a nuevo art qu son y c funcionan las convolutional neural networks esta arquitectura usando capas profundas y la clasificaci de salida abrieron un mundo nuevo de posibilidades en las redes neuronales las cnn se usan tambi en reconocimiento de video y tareas de procesamiento del lenguaje natural quieres hacer tu propia cnn en python 1997 long short term memory recurrent neural network aqui vemos que la red lstm tiene neuronas ocultas con loops hacia atr s esto permite que almacene informaci en celdas de memoria las long short term memory son un tipo de recurrent neural network esta arquitectura permite conexiones hacia atr entre las capas esto las hace buenas para procesar datos de tipo time hist en 1997 se crearon las lstm que consisten en unas celdas de memoria que permiten a la red recordar valores por per cortos o largos una celda de memoria contiene compuertas que administran como la informaci fluye dentro o fuera la puerta de entrada controla cuando puede entran nueva informaci en la memoria la puerta de controla cuanto tiempo existe y se retiene esa informaci la puerta de salida controla cuando la informaci en la celda es usada como salida de la celda la celda contiene pesos que controlan cada compuerta el algoritmo de entrenamiento conocido como backpropagation through time optimiza estos pesos basado en el error de resultado las lstm se han aplicado en reconocimiento de voz de escritura text to speech y otras tareas se alcanza el deep learning 2006 deep belief networks la deep belief network utiliza un autoencoder con restricted boltzmann machines para preentrenar a las neuronas de la red y obtener un mejor resultado final antes de las dbn en 2006 los modelos con o cientos de eran considerados demasiado dif de entrenar con y el uso de las redes neuronales artificiales qued estancado con la creaci de una dbn que logro obtener un mejor resultado en el mnist se devolvi el entusiasmo en poder lograr el aprendizaje profundo en redes neuronales hoy en d las dbn no se utilizan demasiado pero fueron un gran hito en la historia en el desarrollo del deep learning y permitieron seguir la exploraci para mejorar las redes existentes cnn lstm etc las deep belief networks demostraron que utilizar pesos aleatorios al inicializar las redes son una mala idea por ejemplo al utilizar backpropagation con descenso por gradiente muchas veces se ca en m locales sin lograr optimizar los pesos mejor ser utilizar una asignaci de pesos inteligente mediante un preentrenamiento de las capas de la red en ingl se basa en el uso de la utilizaci de restricted boltzmann machines y autoencoders para pre entrenar la red de manera no supervisada ojo luego de pre entrenar y asignar esos pesos iniciales deberemos entrenar la red por de forma habitual supervisada ejemplo con se cree que ese preentrenamiento es una de las causas de la gran mejora en las redes neuronales y permitir el deep learning pues para asignar los valores se eval capa a capa de a una y no de cierto sesgo que causa el backpropagation al entrenar a todas las capas en simult neo 2014 generative adversarial networks las gan entrenan dos redes neuronales en simult neo la red de generaci y la red de discriminaci a medida que la m quina aprende comienza a crear muestras que son indistinguibles de los datos reales estas redes pueden aprender a crear muestras de manera similar a los datos con las que las alimentamos la idea detr s de gan es la de tener dos modelos de redes neuronales compitiendo uno llamado generador toma inicialmente datos como entrada y genera muestras el otro modelo llamado discriminador recibe a la vez muestras del generador y del conjunto de entrenamiento y deber ser capaz de diferenciar entre las dos fuentes estas dos redes juegan una partida continua donde el generador aprende a producir muestras m s realistas y el discriminador aprende a distinguir entre datos reales y muestras artificiales estas redes son entrenadas simult neamente para finalmente lograr que los datos generados no puedan detectarse de datos reales sus aplicaciones principales son la de generaci de im genes realistas pero tambi la de mejorar im genes ya existentes o generar textos en im genes o generar textos siguiendo un estilo determinado y hasta desarrollo de mol para industria farmac conclusi repaso y nuevos descubrimientos hemos recorrido estos primeros casi 80 a de avances en las redes neuronales en la historia de la inteligencia artificial se suele dividir en 3 etapas del 40 al 70 en donde se pas del asombro de estos nuevos modelos hasta el escepticismo el retorno de un invierno de 10 a cuando en los ochentas surgen mejoras en mecanismos y maneras de entrenar las redes backpropagation y se alcanza una meseta en la que no se puede alcanzar la de aprendizaje seguramente tambi por falta de poder de c y una tercer etapa a partir de 2006 en la que se logra superar esa barrera y aprovechando el poder de las gpu y nuevas ideas se logra entrenar cientos de capas jer rquicas que conforman y potencian el deep learning y dan una capacidad casi ilimitada a estas redes como comentario me gustar decir que recientemente feb 2018 hay nuevos estudios de las neuronas humanas biol en las que se est redescubriendo su funcionamiento y se est produciendo una nueva revoluci pues parece que es totalmente distinto a lo que hasta hoy conoc esto puede ser el principio de una nueva etapa totalmente nueva y seguramente mejor del aprendizaje profundo el machine learning y la inteligencia artificial suscripci al blog recibe nuevos art sobre machine learning redes neuronales y c python cada 15 d email deja vac este campo si eres humano m s recursos leyendo qu son y c funcionan las redes neuronales convolutionales cheat sheets for ai neural networks and deep learning from perceptrons to deep networks neural networks achitectures a beginners guide to machine learning a guide on time series prediction using lstm convolutional neural networks in python with keras history of neural network comparte esto haz clic para compartir en twitter abre en una ventana haz clic para compartir en facebook abre en una ventana haz clic para compartir en linkedin abre en una ventana haz clic para compartir en pinterest abre en una ventana haz clic para compartir en telegram abre en una ventana haz clic para compartir en whatsapp abre en una ventana haz clic para enviar por correo electr a un amigo abre en una ventana relacionado categor general etiquetas aprendizaje profundo deep learning definici modelos redes neuronales respuesta a breve historia de las redes neuronales david mart dice 09 2018 a las 09 16 hola juan hace poco te buscaba no te resulta que voy a empezar una asignatura de inteligencia artificial en la ya te ir informando de los avances y te preguntar que veo que eres un crack un abrazo amigo responder dice 09 2018 a las 12 21 hola david gracias por escribirme me est costando conseguir huecos para ponerme a escribir pero me encanta la ia me parece genial que tengas ia de asignatura espero poder ayudarte seguimos en contacto abrazo responder javier dice 11 2018 a las 21 28 gracias por la breve pero muy interesante historia responder dice 11 2018 a las 09 46 hola javier de nada me alegro que te haya interesado esperemos que esta historia continue saludos responder deja un comentario cancelar respuesta navegaci de entradas entrada anterior anterior comprar casa o alquilar naive bayes usando python siguiente entrada siguiente comprende principal component analysis buscar por buscar mini bio juan ignacio bagnato es autor de aprende machine learning un blog en el que colabora con la comunidad de desarrolladores en difundir ejemplos y art para programar tu propia inteligencia artificial conoce m s sobre el autor suscripci escribe tu direcci de correo y recibir s notificaciones de los pr art de aprende machine learning cada 15 d y sin spam email deja vac este campo si eres humano libro recomendado por donde empiezo visita nuestra gu de aprendizaje top posts means en python paso a paso una red neuronal en python desde cero sencilla red neuronal en python con keras y tensorflow lineal en espa con python ambiente de desarrollo python anaconda para aprendizaje autom tico categor general pr ctica redes sociales twitter facebook linkedin pinterest s en twitter mis tuits entradas recientes 12 consejos para aplicar machine learning pron de ventas con redes neuronales parte 2 pron de series temporales con redes neuronales en python machine learning en la nube google colaboratory con gpu ejemplo web scraping en python la bolsa de madrid comentarios recientes loscalzojony en 12 consejos para aplicar machine learning jose flores bendez en clasificaci de im genes en python en principales algoritmos usados en machine learning jose en principales algoritmos usados en machine learning en instalar ambiente de desarrollo python anaconda para aprendizaje autom tico archivos abril 2019 marzo 2019 febrero 2019 enero 2019 diciembre 2018 noviembre 2018 octubre 2018 septiembre 2018 agosto 2018 julio 2018 mayo 2018 abril 2018 marzo 2018 diciembre 2017 noviembre 2017 septiembre 2017 agosto 2017 estoy perdido a donde voy visita nuestro mapa del sitio privacy policy read etiquetas algoritmos aprender aprendizaje autom tico aprendizaje profundo arduino big data clasificaci consejos cursos c data science datos deep learning definici desarrollo ejemplo ejercicio entrenar evaluaci gr ficas jupyter notebook machine learning modelos nlp overfitting pln predicci pron python recursos redes neuronales regresi lineal reviews tutorial visualizaci y cookies este sitio utiliza cookies al continuar utilizando este sitio web aceptas su uso para obtener m s informaci incluido c controlar las cookies consulta aqu de cookie privacy policy con wordpress enviar a direcci de correo electr su nombre tu direcci de correo electr cancelar entrada no fue enviada comprueba tus direcciones de correo electr en la comprobaci de email por favor vuelve a intentarlo sentimos tu blog no puede compartir entradas por correo electr 