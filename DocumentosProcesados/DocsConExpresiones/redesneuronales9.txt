 breve historia de las redes neuronales artificiales aprende machine learning ir al contenido aprende machine learning crea tu propia inteligencia artificial antes que sea demasiado tarde men inicio contacto sobre mi gua de aprendizaje publicado en 12 09 2018 26 02 2019 por breve historia de las redes neuronales artificiales arquitecturas y aplicaciones de las redes neuronales ms usadas vamos a hacer un repaso por las diversas estructuras inventadas mejoradas y utilizadas a lo largo de la historia para crear redes neuronales y sacar el mayor potencial al deep learning para resolver toda clase de problemas de regresin y clasificacin evolucin de las redes neuronales en ciencias de la computacin vamos a revisar las siguientes redes arquitecturas 1958 perceptron 1965 multilayer perceptron s neuronas sigmoidales redes feedforward backpropagation 1989 convolutional neural networks recurent neural networks 1997 long short term memory 2006 deep belief networks nace deep learning restricted boltzmann machine encoder decoder auto encoder 2014 generative adversarial networks si bien esta lista no es exhaustiva y no se abarcan todos los modelos creados desde los aos 50 he recopilado las que fueron a mi parecer las redes y tecnologas ms importantes desarrolladas para llegar al punto en que estamos hoy el aprendizaje profundo el inicio de todo la neurona artificial 1958 perceptron entre las dcadas de 1950 y 1960 el cientfico frank rosenblatt inspirado en el trabajo de warren mcculloch y walter pitts cre el perceptron la unidad desde donde nacera y se potenciaran las redes neuronales artificiales un perceptron toma varias entradas binarias etc y produce una sla salida binaria para calcular la salida rosenblatt introduce el concepto de etc un nmero real que expresa la importancia de la respectiva entrada con la salida la salida de la neurona ser 1 o 0 si la suma de la multiplicacin de pesos por entradas es mayor o menor a un determinado umbral sus principales usos son decisiones binarias sencillas o para crear funciones lgicas como or and 1965 multilayer perceptron como se imaginarn el multilayer perceptron es una del percepcin de una nica neurona a ms de una adems aparece el concepto de capas de entrada oculta y salida pero con valores de entrada y salida binarios no olvidemos que tanto el valor de los pesos como el de umbral de cada neurona lo asignaba manualmente el cientfico cuantos ms perceptrones en las capas mucho ms difcil conseguir los pesos para obtener salidas deseadas los aprendizaje automtico neuronas sigmoides para poder lograr que las redes de neuronas aprendieran solas fue necesario introducir un nuevo tipo de neuronas las llamadas neuronas sigmoides son similares al perceptron pero permiten que las entradas en vez de ser ceros o unos puedan tener valores reales como 0 5 0 377 lo que sea tambin aparecen las neuronas que siempre suman 1 en las diversas capas para resolver ciertas situaciones ahora las salidas en vez de ser 0 1 ser x donde d ser la funcin sigmoide definida como 1 1 z esta es la primer funcin de activacin imagen de la curva logstica normalizada de wikipedia con esta nueva frmula se puede lograr que pequeas alteraciones en valores de los pesos produzcan pequeas alteraciones en la salida por lo tanto podemos ir ajustando muy de a poco los pesos de las conexiones e ir obteniendo las salidas deseadas redes feedforward se les llama as a las redes en que las salidas de una capa son utilizadas como entradas en la prxima capa esto quiere decir que no hay loops hacia siempre se de valores hacia adelante hay redes que veremos ms adelante en las que s que existen esos loops neural adems existe el concepto de fully connected feedforward y se refiere a que todas las neuronas de entrada estn conectadas con todas las neuronas de la siguiente capa 1986 backpropagation gracias al algoritmo de backpropagation se hizo posible entrenar redes neuronales de multiples capas de manera supervisada al calcular el error obtenido en la salida e ir propagando hacia las capas anteriores se van haciendo ajustes pequeos cada iteracin para lograr que la red aprenda consiguiendo que la red pueda por ejemplo clasificar las entradas correctamente nuevo artculo pronstico de series temporales con redes neuronales 1989 convolutional neural network las convolutional neural networks son redes multilayered que toman su inspiracin del cortex visual de los animales esta arquitectura es til en varias aplicaciones principalmente procesamiento de imgenes la primera cnn fue creada por yann lecun y estaba enfocada en el reconocimiento de letras manuscritas la arquitectura constaba de varias capas que implementaban la extraccin de caractersticas y luego clasificar la imagen se divide en campos receptivos que alimentan una capa convolutional que extrae features de la imagen de entrada ejemplo detectar lineas verticales vrtices el siguiente paso es pooling que reduce la dimensionalidad de las features extradas manteniendo la informacin ms importante luego se hace una nueva convolucin y otro pooling que alimenta una red feedforward multicapa la salida final de la red es un grupo de nodos que clasifican el resultado por ejemplo un nodo para cada nmero del 0 al 9 decir 10 nodos se de a nuevo artculo qu son y cmo funcionan las convolutional neural networks esta arquitectura usando capas profundas y la clasificacin de salida abrieron un mundo nuevo de posibilidades en las redes neuronales las cnn se usan tambin en reconocimiento de video y tareas de procesamiento del lenguaje natural quieres hacer tu propia cnn en python 1997 long short term memory recurrent neural network aqui vemos que la red lstm tiene neuronas ocultas con loops hacia atrs esto permite que almacene informacin en celdas de memoria las long short term memory son un tipo de recurrent neural network esta arquitectura permite conexiones hacia entre las capas esto las hace buenas para procesar datos de tipo time en 1997 se crearon las lstm que consisten en unas celdas de memoria que permiten a la red recordar valores por perodos cortos o largos una celda de memoria contiene compuertas que administran como la informacin fluye dentro o fuera la puerta de entrada controla cuando puede entran nueva informacin en la memoria la puerta de controla cuanto tiempo existe y se retiene esa informacin la puerta de salida controla cuando la informacin en la celda es usada como salida de la celda la celda contiene pesos que controlan cada compuerta el algoritmo de entrenamiento conocido como backpropagation through time optimiza estos pesos basado en el error de resultado las lstm se han aplicado en reconocimiento de voz de escritura text to speech y otras tareas se alcanza el deep learning 2006 deep belief networks la deep belief network utiliza un autoencoder con restricted boltzmann machines para preentrenar a las neuronas de la red y obtener un mejor resultado final antes de las dbn en 2006 los modelos con o cientos de eran considerados demasiado difciles de entrenar con y el uso de las redes neuronales artificiales qued estancado con la creacin de una dbn que logro obtener un mejor resultado en el mnist se devolvi el entusiasmo en poder lograr el aprendizaje profundo en redes neuronales hoy en da las dbn no se utilizan demasiado pero fueron un gran hito en la historia en el desarrollo del deep learning y permitieron seguir la exploracin para mejorar las redes existentes cnn lstm etc las deep belief networks demostraron que utilizar pesos aleatorios al inicializar las redes son una mala idea por ejemplo al utilizar backpropagation con descenso por gradiente muchas veces se caa en mnimos locales sin lograr optimizar los pesos mejor ser utilizar una asignacin de pesos inteligente mediante un preentrenamiento de las capas de la red en ingls se basa en el uso de la utilizacin de restricted boltzmann machines y autoencoders para pre entrenar la red de manera no supervisada ojo luego de pre entrenar y asignar esos pesos iniciales deberemos entrenar la red por de forma habitual supervisada ejemplo con se cree que ese preentrenamiento es una de las causas de la gran mejora en las redes neuronales y permitir el deep learning pues para asignar los valores se evala capa a capa de a una y no de cierto sesgo que causa el backpropagation al entrenar a todas las capas en simultneo 2014 generative adversarial networks las gan entrenan dos redes neuronales en simultneo la red de generacin y la red de discriminacin a medida que la mquina aprende comienza a crear muestras que son indistinguibles de los datos reales estas redes pueden aprender a crear muestras de manera similar a los datos con las que las alimentamos la idea detrs de gan es la de tener dos modelos de redes neuronales compitiendo uno llamado generador toma inicialmente datos como entrada y genera muestras el otro modelo llamado discriminador recibe a la vez muestras del generador y del conjunto de entrenamiento y deber ser capaz de diferenciar entre las dos fuentes estas dos redes juegan una partida continua donde el generador aprende a producir muestras ms realistas y el discriminador aprende a distinguir entre datos reales y muestras artificiales estas redes son entrenadas simultneamente para finalmente lograr que los datos generados no puedan detectarse de datos reales sus aplicaciones principales son la de generacin de imgenes realistas pero tambin la de mejorar imgenes ya existentes o generar textos en imgenes o generar textos siguiendo un estilo determinado y hasta desarrollo de molculas para industria farmacutica conclusin repaso y nuevos descubrimientos hemos recorrido estos primeros casi 80 aos de avances en las redes neuronales en la historia de la inteligencia artificial se suele dividir en 3 etapas del 40 al 70 en donde se pas del asombro de estos nuevos modelos hasta el escepticismo el retorno de un invierno de 10 aos cuando en los ochentas surgen mejoras en mecanismos y maneras de entrenar las redes backpropagation y se alcanza una meseta en la que no se puede alcanzar la de aprendizaje seguramente tambin por falta de poder de cmputo y una tercer etapa a partir de 2006 en la que se logra superar esa barrera y aprovechando el poder de las gpu y nuevas ideas se logra entrenar cientos de capas jerrquicas que conforman y potencian el deep learning y dan una capacidad casi ilimitada a estas redes como ltimo comentario me gustara decir que recientemente feb 2018 hay nuevos estudios de las neuronas humanas biolgicas en las que se est redescubriendo su funcionamiento y se est produciendo una nueva revolucin pues parece que es totalmente distinto a lo que hasta hoy conocamos esto puede ser el principio de una nueva etapa totalmente nueva y seguramente mejor del aprendizaje profundo el machine learning y la inteligencia artificial suscripcin al blog recibe nuevos artculos sobre machine learning redes neuronales y cdigo python cada 15 das email deja vaco este campo si eres humano ms recursos seguir leyendo qu son y cmo funcionan las redes neuronales convolutionales cheat sheets for ai neural networks and deep learning from perceptrons to deep networks neural networks achitectures a beginners guide to machine learning a guide on time series prediction using lstm convolutional neural networks in python with keras history of neural network comparte esto haz clic para compartir en twitter abre en una ventana haz clic para compartir en facebook abre en una ventana haz clic para compartir en linkedin abre en una ventana haz clic para compartir en pinterest abre en una ventana haz clic para compartir en telegram abre en una ventana haz clic para compartir en whatsapp abre en una ventana haz clic para enviar por correo electrnico a un amigo abre en una ventana relacionado categoras general etiquetas aprendizaje profundo deep learning definicin modelos redes neuronales 4 respuesta a breve historia de las redes neuronales david martnez dice 12 09 2018 a las 09 16 hola juan hace poco te buscaba no te resulta que voy a empezar una asignatura de inteligencia artificial en la ya te ir informando de los avances y te preguntar que veo que eres un crack un abrazo amigo responder dice 12 09 2018 a las 12 21 hola david gracias por escribirme me est costando conseguir huecos para ponerme a escribir pero me encanta la ia me parece genial que tengas ia de asignatura espero poder ayudarte seguimos en contacto abrazo responder javier dice 23 11 2018 a las 21 28 gracias por la breve pero muy interesante historia responder dice 26 11 2018 a las 09 46 hola javier de nada me alegro que te haya interesado esperemos que esta historia continue saludos responder deja un comentario cancelar respuesta navegacin de entradas entrada anterior anterior comprar casa o alquilar naive bayes usando python siguiente entrada siguiente comprende principal component analysis buscar por buscar mini bio juan ignacio bagnato es autor de aprende machine learning un blog en el que colabora con la comunidad de desarrolladores en difundir ejemplos y artculos para programar tu propia inteligencia artificial conoce ms sobre el autor suscripcin escribe tu direccin de correo y recibirs notificaciones de los prximos artculos de aprende machine learning cada 15 das y sin spam email deja vaco este campo si eres humano libro recomendado por donde empiezo visita nuestra gua de aprendizaje top posts k means en python paso a paso crear una red neuronal en python desde cero una sencilla red neuronal en python con keras y tensorflow regresin lineal en espaol con python instalar ambiente de desarrollo python anaconda para aprendizaje automtico categoras general prctica redes sociales twitter facebook linkedin pinterest sgueme en twitter mis tuits entradas recientes 12 consejos tiles para aplicar machine learning pronstico de ventas con redes neuronales parte 2 pronstico de series temporales con redes neuronales en python machine learning en la nube google colaboratory con gpu ejemplo web scraping en python la bolsa de madrid comentarios recientes loscalzojony en 12 consejos tiles para aplicar machine learning jose flores bendez en clasificacin de imgenes en python en principales algoritmos usados en machine learning jose en principales algoritmos usados en machine learning en instalar ambiente de desarrollo python anaconda para aprendizaje automtico archivos abril 2019 marzo 2019 febrero 2019 enero 2019 diciembre 2018 noviembre 2018 octubre 2018 septiembre 2018 agosto 2018 julio 2018 mayo 2018 abril 2018 marzo 2018 diciembre 2017 noviembre 2017 septiembre 2017 agosto 2017 estoy perdido a donde voy visita nuestro mapa del sitio privacy policy read etiquetas algoritmos aprender aprendizaje automtico aprendizaje profundo arduino big data clasificacin consejos cursos cdigo data science datos deep learning definicin desarrollo ejemplo ejercicio entrenar evaluacin grficas jupyter notebook machine learning modelos nlp overfitting pln prediccin pronstico python recursos redes neuronales regresin lineal reviews tutorial visualizacin privacidad y cookies este sitio utiliza cookies al continuar utilizando este sitio web aceptas su uso para obtener ms informacin incluido cmo controlar las cookies consulta aqu poltica de cookie privacy policy creado con wordpress enviar a direccin de correo electrnico su nombre tu direccin de correo electrnico cancelar la entrada no fue enviada comprueba tus direcciones de correo electrnico error en la comprobacin de email por favor vuelve a intentarlo lo sentimos tu blog no puede compartir entradas por correo electrnico 